{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is class designed for doing data mining for a given specied in NFRI database.Befroe the main functions conducting, I define three classes to help.\n",
    "<p> Firstly, I define a function to connect website process with QDB process. \n",
    "<p> Secondly, I define a function to define specie and state, in other words, to make sure whether it has charge, because it's not listed on the website.\n",
    "<p> Thirdly, I define a function to extract reference information for a given record number.\n",
    "Then, in the main functions:\n",
    "<p> Firstly, I define a function to extract all URL links and their types which are related to electron impact for a given specie.\n",
    "<p> Then, for a given URL link, I retrieve all basic information for recommanded type, including record number, reference information, reaction formula, process, sub process.\n",
    "<p> Finally, for a given record number, I retrieve all the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class creep_species_nfri:\n",
    "    \n",
    "    def __init__(self,specie):\n",
    "        \n",
    "        self.specie = specie\n",
    "        self.pd_data_dict = {}\n",
    "        self.pd_specie = pd.DataFrame(columns=['specie','record_number','process','QDB_process',\\\n",
    "                                               'type','element','ionic_state','initial_state_conf',\\\n",
    "                                               'initial_state','final_state','reaction_formula',\\\n",
    "                                               'x_unit','y_unit','reference_number','author',\\\n",
    "                                               'title_of_record','journal_name','volume_and_issue_No',\\\n",
    "                                               'page_number','date_of_publication'])\n",
    "        \n",
    "    # Define a function to connect website process wtih QDB process.\n",
    "    def define_QDB_process(self,process):\n",
    "        \n",
    "        # Set it empty in case there is no related QDB process.\n",
    "        QDB = ''\n",
    "        \n",
    "        # Judge whether there is a related QDB process, if there is, reset the value of \"QDB\".\n",
    "        if process == 'Electron Impact Total Scattering':\n",
    "            QDB = \"ETS\"\n",
    "        if process == 'Electron Impact Total Ionization':\n",
    "            QDB = \"ETI\"\n",
    "        if process == 'Electron Impact Elastic Scattering':\n",
    "            QDB = \"EEL\"\n",
    "        if process == 'Electron Impact Momentum Transfer  Scattering':\n",
    "            QDB = \"EMT\"       \n",
    "        if process == 'Electron Impact Partial Ionization':\n",
    "            QDB = \"EIN\"\n",
    "        if process == 'Electron Impact Vibrational Excitation':\n",
    "            QDB = \"EVX\"\n",
    "        if process == 'Electron Impact Neutral Product Dissociation Dissociation':\n",
    "            QDB = \"EDS\"\n",
    "        if process == 'Electron Impact Dissociative Attachment':\n",
    "            QDB = \"EDA\"\n",
    "        if process == 'Electron Impact Electronic Excitation':\n",
    "            QDB = \"EEX\"        \n",
    "        if process == 'Electron Impact Rotational Excitation':\n",
    "            QDB = \"ECX\"      \n",
    "        if process == 'Electron Impact Total Attachment':\n",
    "            QDB = \"EDA\" \n",
    "        if process == 'Electron Impact Single Ionization':\n",
    "            QDB = \"EIN\"\n",
    "        if process == 'Electron Impact Dissociative Excitation':\n",
    "            QDB = \"EDE\"        \n",
    "        if process == 'Electron Impact Total Dissociation':\n",
    "            QDB = \"EDS\"\n",
    "        if process == 'Electron Impact Total Excitation':\n",
    "            QDB = \"EEX\"  \n",
    "        if process == 'Electron Impact Total Neutral Fragments Dissociation':\n",
    "            QDB = \"EDS\"  \n",
    "        if process == 'Electron Impact Momentum Transfer Scattering':\n",
    "            QDB = \"EMT\"          \n",
    "        if process == 'Electron Impact Electronic Ionization':\n",
    "            QDB = \"EIN\"  \n",
    "        if process == 'Electron Impact Neutral Fragments Dissociation':\n",
    "            QDB = \"EDS\"  \n",
    "        if process == 'Electron Impact State Selectivity Dissociation':\n",
    "            QDB = \"EDS\"  \n",
    "            \n",
    "        return QDB\n",
    "    \n",
    "    # Define a function to define specie.\n",
    "    def define_specie(self,reaction_formula):\n",
    "        \n",
    "        # Judge whether there is an arrow in the reaction formula, if there is, the products and the final state exists. \n",
    "        # Then, extract the reactants and make formula_1 equal to it.\n",
    "        if re.search('[->]',reaction_formula) == None:\n",
    "            formula_1 = reaction_formula # Define reactants.\n",
    "            final_state = \"\" # Define final_state.\n",
    "        else:\n",
    "            situation = re.search('[-]+[>]',reaction_formula).span()[0] # Find where reactants end.\n",
    "            situation_end = re.search('[-]+[>]',reaction_formula).span()[1] # Find where final state begin.\n",
    "            formula_1 = reaction_formula[:situation+1] # Define reactants.\n",
    "            final_state = reaction_formula[situation_end+1:] # Define final_state.\n",
    "        \n",
    "        # Judge whether there is parenthese in the formula_1, if there is, the initial state conf exists.\n",
    "        # The define specie is the specie begin from the search and before parenthese. \n",
    "        if re.search('[(]',formula_1) == None:\n",
    "            start = re.search(self.specie,formula_1).span()[0] # Search for the given specie and where it starts.\n",
    "            end = re.search(self.specie,formula_1).span()[1] + 1 # Search for the given specie and where it ends.\n",
    "            define_specie = formula_1[start:end] # Define the specie.\n",
    "            initial_state_conf = \"\" # Define initial state conf.\n",
    "        else:\n",
    "            start = re.search(self.specie,formula_1).span()[0] # Search for the given specie and where it starts.\n",
    "            end = re.search('[(]',formula_1).span()[0] # Find where parenthese starts.\n",
    "            define_specie = formula_1[start:end] # Define the specie.\n",
    "            start_1 = re.search('[)]',formula_1).span()[0] # Find where parenthese ends.\n",
    "            initial_state_conf = formula_1[end+1:start_1] # Define initial state conf.\n",
    "         \n",
    "        # Define the inion state by means of searching for \"+\" and \"-\" in the reaction formula.\n",
    "        if \"+\" in define_specie:\n",
    "            ionic_state = 1 # Define that the specie is with positive charge.\n",
    "        elif  \"-\" in define_specie:\n",
    "            ionic_state = -1 # Define that the specie is with negative charge.\n",
    "        else:\n",
    "            ionic_state = 0 # Define that the specie is without charge.\n",
    "        \n",
    "        # Define initial state which includes defined specie and initial state conf.\n",
    "        initial_state = formula_1[start:]\n",
    "\n",
    "        return define_specie,ionic_state,initial_state_conf,initial_state,final_state\n",
    "    \n",
    "    #Define a function to extrac reference information\n",
    "    def extract_DOI(self,No_list_recommended_data):\n",
    "        \n",
    "        # Extract reference number through record number and go the website.\n",
    "        website_basic_information = \"https://dcpp.nfri.re.kr/search/popupViewBasic.do?plBiDataNum=\"\\\n",
    "                                    + No_list_recommended_data[3:] # Get the URL link.\n",
    "        driver_basic_information = webdriver.Chrome()\n",
    "        driver_basic_information.get(website_basic_information) # Go to that website.\n",
    "        \n",
    "        # Get the reference number.\n",
    "        reference_num = driver_basic_information.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[1]/td/a/span\").text\n",
    "        \n",
    "        # Close the website.\n",
    "        driver_basic_information.close()\n",
    "\n",
    "        # Get the reference information website through reference number. \n",
    "        website_reference_link = \"https://dcpp.nfri.re.kr/search/popupViewArticle.do?plRaiArtclNum=\"\\\n",
    "                                    + reference_num # Get the URL link.\n",
    "        driver_reference_link = webdriver.Chrome()\n",
    "        driver_reference_link.get(website_reference_link) # Go to the website.\n",
    "        \n",
    "        # Extract reference information\n",
    "        ## Extract DOI information.\n",
    "        DOI = driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[5]/td\").text\n",
    "        ## Extract title information.\n",
    "        title = driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[2]/td\").text\n",
    "        ## Extract author information.\n",
    "        author = driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[3]/td\").text\n",
    "        author = re.sub(\",\",\" \",author) # Sub \",\" with space.\n",
    "        ## Extract journal name information\n",
    "        journal_name = driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[4]/td\").text\n",
    "        ## Extract volume and issue number information.\n",
    "        volume_and_issue_No = driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[10]/td\").text\\\n",
    "                              + driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[7]/td\").text\n",
    "        ## Extract page number information.\n",
    "        page_number = driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[12]/td\").text\\\n",
    "                      + \"/\" +  driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[13]/td\").text\n",
    "        ## Extract date of publication information.\n",
    "        date_of_publication =  driver_reference_link.find_element_by_xpath(\"/html/body/div[2]/table/tbody/tr[14]/td\").text\n",
    "\n",
    "        # Close the website.\n",
    "        driver_reference_link.close()\n",
    "\n",
    "        return DOI,title,author,journal_name,volume_and_issue_No,page_number,date_of_publication,reference_num\n",
    "    \n",
    "    # Record all URL links for a given specie.\n",
    "    def get_website(self):\n",
    "        \n",
    "        # Get through the website where I find the data\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(\"https://dcpp.nfri.re.kr/index.do\")\n",
    "        \n",
    "        # Enter the species we want to find \n",
    "        driver.find_element_by_id('keyword').send_keys(self.specie)\n",
    "        sleep(2) # Sleep in case that the website hasn't opened.\n",
    "        driver.find_element_by_id('searchimg').click()\n",
    "        sleep(2) # Sleep in case that the website hasn't opened.    \n",
    "        \n",
    "        # Find the cross section id\n",
    "        ## Find URL links which are related with electron impact.\n",
    "        urls = driver.find_elements_by_xpath(\"//div[@id='containerPlBiDataBranchDB_02-MP_01-IC_01']/a\")\n",
    "        \n",
    "        # Extract cross section id, URL links and type.\n",
    "        cross_section_id = []\n",
    "        cross_section_urls = []\n",
    "        cross_section_type = []\n",
    "        ## Get the attribute \"ID\" of each URL link.\n",
    "        if len(urls) != 0:\n",
    "            for url in urls:\n",
    "                id = url.get_attribute(\"id\")\n",
    "                cross_section_id.append(id)\n",
    "\n",
    "            # Through cross section id I can find all the links\n",
    "            len_cross_section_id = len(cross_section_id)\n",
    "            if len_cross_section_id != 0: # If it is equal to 0, it means there is no URL links.\n",
    "                for i in range (len_cross_section_id):\n",
    "                    # if the length is more than 4, I need to page turning to get URL links\n",
    "                    if i > 4:\n",
    "                        driver.find_element_by_id('btnPlBiDataBranchNext:DB_02-MP_01-IC_01').click() # Find page turning icon and click it.\n",
    "                    driver.find_element_by_id(cross_section_id[i]).click()\n",
    "                    # Get the current URL link and record it.\n",
    "                    now_url = driver.current_url\n",
    "                    cross_section_urls.append(now_url)\n",
    "                    driver.back()\n",
    "                    \n",
    "                # Define cross section type through cross section id\n",
    "                for i in range (len_cross_section_id):\n",
    "                    type_xpath = \"//a[@id='\"+cross_section_id[i]+\"']/strong\" # Find type name location through id.\n",
    "                    type_name = driver.find_element_by_xpath(type_xpath).text # Extract type name.\n",
    "                    cross_section_type.append(type_name)\n",
    "                    \n",
    "            # Close the website.\n",
    "            driver.close()\n",
    "        else:\n",
    "            # Close the website.\n",
    "            driver.close()\n",
    "          \n",
    "        # Memorize cross section informaion in the class.\n",
    "        self.cross_section_urls = cross_section_urls\n",
    "        self.cross_section_type = cross_section_type\n",
    "     \n",
    "    # Define a function to extract basic information\n",
    "    def extract_pd_specie(self,url,type_name):\n",
    "        \n",
    "        # Go to the website for a given cross section type which is connected with URL link.\n",
    "        driver_specie = webdriver.Chrome()\n",
    "        driver_specie.get(url)\n",
    "        \n",
    "        # Extract units information.\n",
    "        ## Find x unit\n",
    "        x_label = driver_specie.find_elements_by_class_name(\"nv-axislabel\")[0].text # Find the location of x unit.\n",
    "        if re.search('[[]',x_label) != None:\n",
    "            x_begin = re.search('[[]',x_label).span()[1] # Find the start of x unit.\n",
    "            x_end = re.search('[]]',x_label).span()[0] # Find the end of x unit.\n",
    "            x_unit = x_label[x_begin:x_end]\n",
    "        else:\n",
    "            x_unit = 'eV' # If the figure doesn't show x label, it will be assigned \"eV\".\n",
    "        ## Find y unit   \n",
    "        y_label = driver_specie.find_elements_by_class_name(\"nv-axislabel\")[1].text # Find the location of y unit.\n",
    "        if re.search('[[]',y_label) != None:\n",
    "            y_begin = re.search('[[]',y_label).span()[1] # Find the start of y unit.\n",
    "            y_end = re.search('[]]',y_label).span()[0] # Find the end of y unit.\n",
    "            y_unit = y_label[y_begin:y_end]\n",
    "        else:\n",
    "            y_unit = '1E-16 cm^2' # If the figure doesn't show y label, it will be assigned \"1E-16 cm^2\".\n",
    "        \n",
    "        # Extract all the record number for a given type\n",
    "        No_list_xpath = driver_specie.find_elements_by_xpath(\"//table[@class='table_list datatable']/tbody/tr\") # Find record number location.\n",
    "        No_list = []\n",
    "        for No in No_list_xpath:\n",
    "            No_list.append(No.get_attribute(\"id\")) # Record \"ID\" to judge whether it is recommanded later.\n",
    "\n",
    "        # Extract recommended data record number.\n",
    "        No_list_recommended_data = []\n",
    "        for i in range (len(No_list)):\n",
    "            xpath = \"//tr[@id='\"+No_list[i]+\"']/td[8]\" # Find theory location.\n",
    "            ## Get theory and in case the text is hidden, I use \"innerText\".\n",
    "            theory = driver_specie.find_elements_by_xpath(xpath)[0].get_attribute(\"innerText\")\n",
    "            if theory == \"Recommended Data\": # If theory is recommaned, append record number.\n",
    "                No_list_recommended_data.append(No_list[i])\n",
    "        \n",
    "        # Extract basic information for each record number.\n",
    "        for i in range (len(No_list_recommended_data)):\n",
    "            ## Define current record number.\n",
    "            ID = No_list_recommended_data[i]\n",
    "            ## Extract meta data for a given ID.\n",
    "            result_DOI = self.extract_DOI(ID)\n",
    "            ## Define type.\n",
    "            type = \"R\"\n",
    "            \n",
    "            # Extract reaction formula.\n",
    "            xpath_reaction_formula = \"//tr[@id='\"+ID+\"']/td[5]\" # Find reaction formula location.\n",
    "            reaction_formula = driver_specie.find_elements_by_xpath(xpath_reaction_formula)[0].get_attribute(\"innerText\") # Get reaction formula.\n",
    "    \n",
    "            #extract collision method.\n",
    "            xpath_collision_method = \"//tr[@id='\"+ID+\"']/td[6]\" # Find collision method.\n",
    "            collision_method = driver_specie.find_elements_by_xpath(xpath_collision_method)[0].get_attribute(\"innerText\") # Get collision method.\n",
    "            \n",
    "            # Extract sub process\n",
    "            xpath_sub_process = \"//tr[@id='\"+ID+\"']/td[7]\" # Find sub process location.\n",
    "            sub_process = driver_specie.find_elements_by_xpath(xpath_sub_process)[0].get_attribute(\"innerText\") # Get sub process.\n",
    "            \n",
    "            # Define specie name, ionic state, initial state conf, initial state and final state through define specie function.\n",
    "            specie_name = self.define_specie(reaction_formula)[0]\n",
    "            ionic_state = self.define_specie(reaction_formula)[1]\n",
    "            initial_state_conf = self.define_specie(reaction_formula)[2]\n",
    "            initial_state = self.define_specie(reaction_formula)[3]\n",
    "            final_state = self.define_specie(reaction_formula)[4]\n",
    "            \n",
    "            # Combine collision method, sub process and type name into process\n",
    "            process = collision_method + \" \" + sub_process + \" \" + type_name\n",
    "            QDB_process = self.define_QDB_process(process)\n",
    "            \n",
    "            # Memorize the record and add it into the dataframe.\n",
    "            pd_single_specie = pd.DataFrame(data = [specie_name,ID,process,QDB_process,\\\n",
    "                                                    type,self.specie,ionic_state,initial_state_conf,\\\n",
    "                                                    initial_state,final_state,reaction_formula,\\\n",
    "                                                    x_unit,y_unit,result_DOI[7],result_DOI[2],\\\n",
    "                                                    result_DOI[1],result_DOI[3],result_DOI[4],\\\n",
    "                                                    result_DOI[5],result_DOI[6],result_DOI[0]],\\\n",
    "                                           index = ['specie','record_number','process','QDB_process',\\\n",
    "                                                    'type','element','ionic_state','initial_state_conf',\\\n",
    "                                                    'initial_state','final_state','reaction_formula',\\\n",
    "                                                    'x_unit','y_unit','reference_number','author',\\\n",
    "                                                    'title_of_record','journal_name','volume_and_issue_No',\\\n",
    "                                                    'page_number','date_of_publication','DOI']).T\n",
    "            self.pd_specie = self.pd_specie.append(pd_single_specie,sort=False)\n",
    "        \n",
    "        # Close the website.\n",
    "        driver_specie.close()\n",
    "    \n",
    "    # Define a function to extract meta data.\n",
    "    def extract_data(self):\n",
    "        \n",
    "        # Get all cross section URL links.\n",
    "        self.get_website()\n",
    "        \n",
    "        if len(self.cross_section_urls) != 0:\n",
    "        \n",
    "            for i in range (len(self.cross_section_urls)):\n",
    "                url = self.cross_section_urls[i]\n",
    "                ## Extract type name.\n",
    "                type_name = self.cross_section_type[i]\n",
    "                ## Extract basic information for a given type.\n",
    "                self.extract_pd_specie(url,type_name)\n",
    "\n",
    "            # Get record number list.\n",
    "            ID_list = self.pd_specie['record_number'].tolist()\n",
    "\n",
    "            # Extract meta data for each record number.\n",
    "            for j in range (len(ID_list)):\n",
    "                ## Get the URL link.\n",
    "                website_view_data = \"https://dcpp.nfri.re.kr/search/popupView.do?type=numerical&plBiDataNum=\"+str(ID_list[j][3:])\n",
    "                ## Get into the website.\n",
    "                driver_view_data = webdriver.Chrome()\n",
    "                driver_view_data.get(website_view_data)\n",
    "                ## Count number of meta data\n",
    "                cout = len(driver_view_data.find_elements_by_xpath(\"//tbody[@id='tbody']/tr\"))\n",
    "\n",
    "                X = []\n",
    "                Y = []\n",
    "                X_error = []\n",
    "                Y_error = []\n",
    "\n",
    "                ## Extract meta data\n",
    "                for i in range (cout):\n",
    "                    ### Find positions\n",
    "                    xpath_x = \"//tbody[@id='tbody']/tr[\" + str(i + 1) + \"]/td[1]\"\n",
    "                    xpath_y = \"//tbody[@id='tbody']/tr[\" + str(i + 1) + \"]/td[2]\"\n",
    "                    xpath_x_err = \"//tbody[@id='tbody']/tr[\" + str(i + 1) + \"]/td[3]\"\n",
    "                    xpath_y_max_err = \"//tbody[@id='tbody']/tr[\" + str(i + 1) + \"]/td[4]\"\n",
    "                    xpath_y_min_err = \"//tbody[@id='tbody']/tr[\" + str(i + 1) + \"]/td[5]\"\n",
    "\n",
    "                    ### Record data\n",
    "                    x_value = driver_view_data.find_elements_by_xpath(xpath_x)[0].text\n",
    "                    y_value = driver_view_data.find_elements_by_xpath(xpath_y)[0].text\n",
    "                    x_err = driver_view_data.find_elements_by_xpath(xpath_x_err)[0].text\n",
    "                    y_max_err = driver_view_data.find_elements_by_xpath(xpath_y_max_err)[0].text\n",
    "                    y_min_err = driver_view_data.find_elements_by_xpath(xpath_y_min_err)[0].text\n",
    "                    \n",
    "                    ### Judge whether y max error is equal to y min error. If it's true, just keep one of them, otherwise keep them both.\n",
    "                    if float(y_max_err) == float(y_max_err):\n",
    "                        y_err = y_max_err\n",
    "                    else:\n",
    "                        y_err = \"Max:\" + y_max_err + \"/Min:\" + y_min_err\n",
    "\n",
    "\n",
    "                    X.append(x_value)\n",
    "                    Y.append(y_value)\n",
    "                    X_error.append(x_err)\n",
    "                    Y_error.append(y_max_err)\n",
    "\n",
    "                # Close the website.\n",
    "                driver_view_data.close()\n",
    "\n",
    "                # Memorize meta data dataframe and build a library to record it for each ID.\n",
    "                pd_data = pd.DataFrame(data = [X,Y,X_error,Y_error],\\\n",
    "                                       index = ['X','Y','X_error','Y_error']).T\n",
    "                pd_data = pd_data.reindex(columns=['ID','X','Y','X_error','Y_error'],\\\n",
    "                                                   fill_value=ID_list[j])\n",
    "                self.pd_data_dict[ID_list[j]] = pd_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
