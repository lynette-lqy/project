{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import sys\n",
    "import creeper_nifs\n",
    "import creeper_nfri\n",
    "import delete_duplicate_data\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Do NLP for species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I define a function to remove charge in the ordinary formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove charge.\n",
    "def NLP_species(specie):\n",
    "    # Remove positive and negative charge in the ordinary formula.\n",
    "    r_1 = '[+]'\n",
    "    r_2 = '-'\n",
    "    output = re.sub(r_1,'',specie)\n",
    "    output = re.sub(r_2,'',output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I extract original file of species list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file and make it a dataframe.\n",
    "qdb_species_list = []\n",
    "with open('qdb_species.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        qdb_species_list.append(row)\n",
    "qdb_species = pd.DataFrame(qdb_species_list[1:], \\\n",
    "                           columns=qdb_species_list[0]) \n",
    "qdb_species = qdb_species.drop(['mass', 'energy', 'hform', \\\n",
    "                                'lj_epsilon', 'lj_sigma'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by using the function above, I can get a list of species without charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuqingyang/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Add column which record species without charge.\n",
    "qdb_species['ordinary_formula_without_charge'] = 0\n",
    "len_species = len(qdb_species)\n",
    "for i in range (len_species):\n",
    "    qdb_species['ordinary_formula_without_charge'].iloc[i] = NLP_species(qdb_species['ordinary_formula'].iloc[i])\n",
    "# Extract species and avoid duplicate species.\n",
    "species_list = list(set(qdb_species['ordinary_formula_without_charge'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Data mining for NFRI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'send_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a420bd9461f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcps_nfri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreeper_nfri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreep_species_nfri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcps_nfri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mcs_nifr_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcps_nfri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_specie\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcps_nfri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_data_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project/code/creeper_nfri.py\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# Get all cross section URL links.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_section_urls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project/code/creeper_nfri.py\u001b[0m in \u001b[0;36mget_website\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Enter the species we want to find\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sleep in case that the website hasn't opened.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'searchimg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'send_keys'"
     ]
    }
   ],
   "source": [
    "# Do data mining for NFRI database and save basic information data and meta data for each specie.\n",
    "cs_nifr_list = []\n",
    "for i in range (len(species_list)):\n",
    "        cps_nfri = creeper_nfri.creep_species_nfri(species_list[i])\n",
    "        cps_nfri.extract_data()\n",
    "        cs_nifr_list.append([cps_nfri.pd_specie,cps_nfri.pd_data_dict])\n",
    "\n",
    "# Combine basic information data for all species.       \n",
    "pd_nfri_main = pd.DataFrame(index=['specie','record_number','process','QDB_process',\\\n",
    "                                   'type','element','ionic_state','initial_state_conf',\\\n",
    "                                   'initial_state','final_state','reaction_formula',\\\n",
    "                                   'x_unit','y_unit','reference_number','author',\\\n",
    "                                   'title_of_record','journal_name','volume_and_issue_No',\\\n",
    "                                   'page_number','date_of_publication','DOI']).T\n",
    "for i in range (len(cs_nifr_list)):\n",
    "    pd_nfri_main = pd_nfri_main.append(cs_nifr_list[i][0])\n",
    "# Save the file.\n",
    "pd_nfri_main.to_csv('pd_nfri_main.csv',sep='\\t',index=False)\n",
    "\n",
    "# Combine meta data for all record numbers\n",
    "pd_nfri_data = pd.DataFrame(columns=['ID','X','Y','X_error','Y_error'])\n",
    "for i in range (len(cs_nifr_list)):\n",
    "    if len(cs_nifr_list[i][0])!=0: # If the length of meta data is 0, I don't need to append it.\n",
    "        ID = cs_nifr_list[i][0]['record_number'].tolist()\n",
    "        for j in range (len(ID)):\n",
    "            if len(cs_nifr_list[i][1])!=0:\n",
    "                pd_nfri_data = pd_nfri_data.append(cs_nifr_list_5[i][1][ID[j]])\n",
    "                ## Save single file for each record number.\n",
    "                cs_nifr_list[i][1][ID[j]].to_csv(ID[j]+'.csv',sep='\\t',index=False)\n",
    "# Save the whole meta data file.\n",
    "pd_nfri_data.to_csv('pd_nfri_data.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Data mining for NIFS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I extract nifs species list to find common species in both databases.\n",
    "## Open the file of nifs species.\n",
    "nifs_species_list = []\n",
    "with open('nifs_species.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        nifs_species_list.append(row[0])\n",
    "nifs_species_list = nifs_species_list[1:]\n",
    "nifs_species_list = list(set(nifs_species_list))\n",
    "\n",
    "# Find common species in QDB specie list and NIFS database.\n",
    "nifs_qdb_species = []\n",
    "for i in range (len(nifs_species_list)):\n",
    "    if nifs_species_list[i] in species_list:\n",
    "        nifs_qdb_species.append(nifs_species_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do data mining for NIFS database and save basic information data and meta data for each specie.\n",
    "cs_nifs_list = []\n",
    "for i in range (len(nifs_qdb_species)):\n",
    "    cs_nifs = creeper_nifs.creep_species_nifs(nifs_qdb_species[i].strip())\n",
    "    cs_nifs.extract_basic_information()\n",
    "    cs_nifs.extract_data()\n",
    "    cs_nifs_list.append([cs_nifs.pd_specie,cs_nifs.data_list])\n",
    "\n",
    "# Combine basic information data for all species.\n",
    "pd_nifs_main = pd.DataFrame(columns=['specie','record_number','process','QDB_process','type','element',\\\n",
    "                                    'ionic_state','initial_state_conf','initial_state',\\\n",
    "                                    'final_state','reaction_formula','x_unit','y_unit',\\\n",
    "                                    'reference_number','author','title_of_record','journal_name',\\\n",
    "                                    'volume_and_issue_No','page_number','date_of_publication'])\n",
    "for i in range (len(cs_nifs_list)):\n",
    "    pd_nifs_main = pd_nifs_main.append(cs_nifs_list[i][0])\n",
    "# Save the file.\n",
    "pd_nifs_main.to_csv('pd_nifs_main.csv',sep='\\t',index=False)\n",
    "\n",
    "# Combine meta data for all record numbers\n",
    "pd_nifs_data = pd.DataFrame(columns=['ID','X','Y','X_error','Y_error'])\n",
    "for i in range (len(cs_nifs_list)):\n",
    "    ID = cs_nifs_list[i][0]['record_number'].tolist()\n",
    "    for j in range (len(ID)):\n",
    "        if len(cs_nifs_list[i][1])!=0: # If the length of meta data is 0, I don't need to append it.\n",
    "            pd_nifs_data = pd_nifs_data.append(cs_nifs_list[i][1][ID[j]])\n",
    "            ## Save single file for each record number.\n",
    "            cs_nifs_list[i][1][ID[j]].to_csv(ID[j]+'.csv',sep='\\t',index=False)\n",
    "# Save the whole meta data file.\n",
    "pd_nifs_data.to_csv('pd_nifs_data.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Delete duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract NFRI meta data.\n",
    "pd_nfri_data = []\n",
    "with open(\"pd_nfri_data.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for line in csvreader:\n",
    "        pd_nfri_data.append(line[:])\n",
    "pd_nfri_data = pd.DataFrame(data=pd_nfri_data[1:],columns=pd_nfri_data[0])\n",
    "# Change the format of x value.\n",
    "for i in range (len(pd_nfri_data)):\n",
    "    pd_nfri_data['X'].iloc[i] = Decimal(pd_nfri_data['X'].iloc[i]).normalize()\n",
    "    \n",
    "# Extract NIFS meta data\n",
    "pd_nifs_data = []\n",
    "with open(\"pd_nifs_data.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for line in csvreader:\n",
    "        pd_nifs_data.append(line[:])\n",
    "pd_nifs_data = pd.DataFrame(data=pd_nifs_data[1:],columns=pd_nifs_data[0])\n",
    "# Change the format of x value.\n",
    "for i in range (len(pd_nifs_data)):\n",
    "    pd_nifs_data['X'].iloc[i] = Decimal(pd_nifs_data['X'].iloc[i]).normalize()\n",
    "    \n",
    "# Extract NFRI basic information data.\n",
    "pd_nfri_main = []\n",
    "with open(\"pd_nfri_main.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for line in csvreader:\n",
    "        pd_nfri_main.append(line[:])\n",
    "pd_nfri_main = pd.DataFrame(data=pd_nfri_main[1:],columns=pd_nfri_main[0])\n",
    "# Extract NFRI species list.\n",
    "nfri_specie = list(set(pd_nfri_main['specie'].tolist()))\n",
    "for i in range (len(nfri_specie)):\n",
    "    nfri_specie[i] = nfri_specie[i].strip()\n",
    "\n",
    "# Extract NIFS basic information data.\n",
    "pd_nifs_main = []\n",
    "with open(\"pd_nifs_main.csv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "    for line in csvreader:\n",
    "        pd_nifs_main.append(line[:])\n",
    "pd_nifs_main = pd.DataFrame(data=pd_nifs_main[1:],columns=pd_nifs_main[0])\n",
    "# Extract NIFS species list.\n",
    "nifs_specie = list(set(pd_nifs_main['specie'].tolist()))\n",
    "for i in range (len(nifs_specie)):\n",
    "    nifs_specie[i] = nifs_specie[i].strip()\n",
    "\n",
    "# Find common species of two databases.\n",
    "specie_common = []\n",
    "for i in range (len(nifs_specie)):\n",
    "    if nifs_specie[i] in nfri_specie:\n",
    "        specie_common.append(nifs_specie[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common record numbers in two databases.\n",
    "common_results = {}\n",
    "for i in range (len(specie_common)):\n",
    "    ddd = delete_duplicate_data.delete_duplicate_data(specie_common[i])\n",
    "    common_result = ddd.search_title()\n",
    "    if len(common_result) != 0: # I only need to record common species which have common meta data.\n",
    "        common_results[specie_common[i]] = common_result\n",
    "\n",
    "# Record record numbers which need to be deleted in NIFS database.\n",
    "delete_record_number = []\n",
    "list_common_keys = list(common_results.keys())\n",
    "for i in range (len(list_common_keys)):\n",
    "    common_result = common_results[list_common_keys[i]]\n",
    "    for j in range (len(common_result)):\n",
    "        delete_record_number.append(common_result[j][1])\n",
    "delete_record_number = list(set(delete_record_number))\n",
    "\n",
    "# Find record numbers that are not in delete_record_number list.\n",
    "nifs_main_record_number = list(set(pd_nifs_main.record_number))\n",
    "nifs_rest_record_number = []\n",
    "for i in range (len(nifs_main_record_number)):\n",
    "    if nifs_main_record_number[i] not in delete_record_number:\n",
    "        nifs_rest_record_number.append(nifs_main_record_number[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate basic information data and meta data in NIFS database, and combine two databases together.\n",
    "pd_nifs_main_rest = pd_nifs_main[pd_nifs_main.record_number.isin(nifs_rest_record_number)]\n",
    "pd_nifs_main_rest = pd_nifs_main_rest.reindex(columns=['specie','record_number','process','QDB_process',\\\n",
    "                                   'type','element','ionic_state','initial_state_conf',\\\n",
    "                                   'initial_state','final_state','reaction_formula',\\\n",
    "                                   'x_unit','y_unit','reference_number','author',\\\n",
    "                                   'title_of_record','journal_name','volume_and_issue_No',\\\n",
    "                                   'page_number','date_of_publication','DOI'],fill_value='')\n",
    "main = pd_nfri_main.append(pd_nifs_main_rest)\n",
    "# Save the file.\n",
    "main.to_csv('main.csv',sep='\\t',index=False)\n",
    "\n",
    "pd_nifs_data_rest = pd_nifs_data[pd_nifs_data.ID.isin(nifs_rest_record_number)]\n",
    "data = pd_nfri_data.append(pd_nifs_data_rest)\n",
    "# Save the file.\n",
    "data.to_csv('data.csv',sep='\\t',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
